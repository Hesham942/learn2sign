<p align="center">
  <img src="logo.png" alt="Learn2Sign" width="200"/>
</p>

<h1 align="center">Learn2Sign</h1>

<p align="center">
  <strong>Real-time ASL Recognition Mobile App</strong><br>
  Lightweight â€¢ Power-Efficient â€¢ Accurate
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.11+-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python"/>
  <img src="https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white" alt="FastAPI"/>
  <img src="https://img.shields.io/badge/React_Native-61DAFB?style=for-the-badge&logo=react&logoColor=black" alt="React Native"/>
  <img src="https://img.shields.io/badge/Expo-000020?style=for-the-badge&logo=expo&logoColor=white" alt="Expo"/>
  <img src="https://img.shields.io/badge/MediaPipe-4285F4?style=for-the-badge&logo=google&logoColor=white" alt="MediaPipe"/>
  <img src="https://img.shields.io/badge/ONNX-005CED?style=for-the-badge&logo=onnx&logoColor=white" alt="ONNX"/>
  <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white" alt="PyTorch"/>
</p>

<p align="center">
  <a href="#features">Features</a> â€¢
  <a href="#demo">Demo</a> â€¢
  <a href="#architecture">Architecture</a> â€¢
  <a href="#installation">Installation</a> â€¢
  <a href="#usage">Usage</a> â€¢
  <a href="#dataset">Dataset</a> â€¢
  <a href="#tech-stack">Tech Stack</a>
</p>

---

## Demo


https://github.com/user-attachments/assets/e5b94db6-c8bb-4a7f-941b-4e3a811f5d5f


> *Real-time ASL letter recognition with instant feedback*

---

## Features

ğŸ¤Ÿ **Real-time Recognition** â€” Instant ASL letter detection with confidence scores

ğŸ“± **Mobile-First Design** â€” Native Android app built with React Native & Expo

âš¡ **Power Efficient** â€” Landmark-based approach uses 95% less data than image-based models

ğŸ“ **Learning Mode** â€” Interactive tutorials, practice sessions, and challenge quizzes

ğŸ”’ **Privacy Focused** â€” All processing happens on your local network

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Mobile App    â”‚  HTTP   â”‚  FastAPI Server â”‚  ONNX   â”‚   ML Model      â”‚
â”‚  (React Native) â”‚ â”€â”€â”€â”€â”€â”€â–º â”‚   (Python)      â”‚ â”€â”€â”€â”€â”€â”€â–º â”‚  (A-Z Letters)  â”‚
â”‚                 â”‚ base64  â”‚                 â”‚         â”‚                 â”‚
â”‚  ğŸ“· Camera      â”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚  ğŸ–ï¸ MediaPipe   â”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚  ğŸ§  Classifier  â”‚
â”‚  ğŸ¨ UI/UX       â”‚  JSON   â”‚  ğŸ“Š Landmarks   â”‚  pred   â”‚  ğŸ“ˆ Confidence  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How It Works

1. **Capture** â€” Mobile app captures camera frame
2. **Extract** â€” MediaPipe detects 21 hand landmarks (63 features)
3. **Classify** â€” ONNX model predicts ASL letter
4. **Display** â€” Result shown with confidence score

---

## Installation

### Prerequisites

- Python 3.11+
- Node.js 18+
- Android device with Expo Go app
- Both devices on same WiFi network

### Backend Setup

```bash
cd backend
pip install -r requirements.txt
python main.py
```

Server starts at `http://0.0.0.0:8000`

### Mobile Setup

```bash
cd mobile
npm install
npx expo start
```

Scan QR code with Expo Go app.

---

## Usage

1. **Start Backend** â€” Run `python main.py` in backend folder
2. **Find Your IP** â€” Run `ipconfig` (Windows) or `ifconfig` (Mac/Linux)
3. **Launch App** â€” Open mobile app and enter your computer's IP
4. **Connect** â€” Tap "Connect" to establish connection
5. **Sign Away** â€” Show ASL letters to camera for recognition

### App Modes

| Mode | Description |
|------|-------------|
| **Recognize** | Real-time letter detection with camera |
| **Learn** | Step-by-step tutorials for each letter |
| **Practice** | Guided practice with feedback |
| **Challenge** | Test your skills with timed quizzes |

---

## Dataset

### Why We Built Our Own

Most public ASL datasets contain **full hand images**, which require:
- Heavy image processing
- Large model sizes
- High battery consumption
- Slower inference times

### Our Approach

We manually collected a **landmark-based dataset** containing:

| Feature | Value |
|---------|-------|
| Data Points | 63 features per sample (21 landmarks Ã— 3 coordinates) |
| Labels | A-Z (26 classes) |
| Collection | Manual capture via webcam |
| Format | CSV with normalized coordinates |

### Benefits

âœ… **95% smaller** than image datasets  
âœ… **10x faster** inference  
âœ… **Battery efficient** for mobile devices  
âœ… **Privacy preserving** â€” no images stored  

This makes SignLingo uniquely suited for mobile deployment where power efficiency matters.

---

## Tech Stack

### Backend
| Technology | Purpose |
|------------|---------|
| FastAPI | REST API framework |
| MediaPipe | Hand landmark detection |
| ONNX Runtime | Model inference |
| OpenCV | Image preprocessing |

### Mobile
| Technology | Purpose |
|------------|---------|
| React Native | Cross-platform UI |
| Expo | Development & build tooling |
| expo-camera | Camera capture |
| axios | HTTP client |

### ML Pipeline
| Technology | Purpose |
|------------|---------|
| scikit-learn | Model training |
| ONNX | Model export & inference |
| Jupyter | Experimentation |

---

## Project Structure

```
learn2sign/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI endpoints
â”‚   â”œâ”€â”€ classifier.py        # ONNX model wrapper
â”‚   â”œâ”€â”€ landmark_extractor.py # MediaPipe integration
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ mobile/
â”‚   â”œâ”€â”€ App.js               # Navigation setup
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ screens/         # App screens
â”‚       â””â”€â”€ api.js           # Backend client
â”œâ”€â”€ ASL_Clean_Pipeline.ipynb    # Model training notebook
â”œâ”€â”€ asl_model_clean.onnx     # Trained model
â”œâ”€â”€ asl_label_encoder.pkl    # Label mapping
â””â”€â”€ dataset.csv              # Training data
```

---

## API Reference

### Health Check
```http
GET /health
```
```json
{ "status": "healthy" }
```

### Predict Letter
```http
POST /predict
Content-Type: application/json

{ "image": "<base64_encoded_image>" }
```
```json
{
  "letter": "A",
  "confidence": 0.95,
  "landmarks_detected": true,
  "processing_time_ms": 45.2,
  "top_predictions": [
    { "letter": "A", "confidence": 0.95 },
    { "letter": "S", "confidence": 0.03 },
    { "letter": "E", "confidence": 0.01 }
  ]
}
```

---

## Performance

| Metric | Value |
|--------|-------|
| Inference Time | ~50ms |
| Model Size | 2.1 MB |
| Accuracy | 98%+ |
| Battery Impact | Minimal |

---

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## License

This project is licensed under the MIT License.

---

<p align="center">
  Made with â¤ï¸ for the deaf and hard-of-hearing community
</p>
